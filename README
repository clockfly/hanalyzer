Hadoop job history analyzer

This script can parse hadoop job history log files, and generate csv/json 
reports, basically you can use it to get execution times, counters, status 
of jobs, statistics about tasks and attempts, etc.

This module can be used as a module to parse job history repository, or used 
as a utility to generate job statistics report to get brief information about 
the whole cluster.

Usage: job_history.py [options] <repo path>

Sample: job_history.py  -s 2012-09-09 -e 2013-09-10  /var/log/hadoop/history/
Will extract log of date 2012-09-09.

Options:
  -h, --help            show this help message and exit
  -t TYPE, --type=TYPE  output file type, json|csv|all(output all formats)
                        default: all
  -o OUTPUT, --output=OUTPUT
                        output file prefix, default: jobstats
  -s STARTDATE, --start=STARTDATE
                        start date, e.g. 2012-09-09 default: None. The start date is inclusive.
  -e ENDDATE, --end=ENDDATE
                        end date e.g. 2012-09-09 default: None. The end date is exclusive.
  -n, --dry-run         just scan repo and estimate output size and running
                        time
  -f, --flat            history repo is old Hadoop 0.20 style flat directory,
                        one monster dir with huge number of history files
  -p PERIOD, --period=PERIOD
                        output one file with each period, day|week|month|none
Report data include:

'date',
'jobname', 
'jobid', 
'user', 
'job_priority', 
'job_status', 
'total_maps', 
'total_reduces', 
'failed_maps', 
'failed_reduces', 
'submit_time', 
'launch_time', 
'finish_time', 
'map_avg_time', 
'reduce_avg_time', 
'hdfs_bytes_read', 
'hdfs_bytes_written', 
'map_output_bytes',

For more details on output data, take look at example_output.csv and 
example_output.json.


